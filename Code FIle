{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Automated Essay Evaluation \n",
        "#####Chinmay Pagey\n",
        "#####Chandan Kumar Sangewar"
      ],
      "metadata": {
        "id": "s00dC6yPqv8C"
      },
      "id": "s00dC6yPqv8C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ca47f9f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter \n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "id": "4ca47f9f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1915ada"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train.csv')\n",
        "df_train.head()"
      ],
      "id": "a1915ada"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c48205b"
      },
      "outputs": [],
      "source": [
        "df_train.info()"
      ],
      "id": "5c48205b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04771860"
      },
      "outputs": [],
      "source": [
        "df_train.isna().sum()"
      ],
      "id": "04771860"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5467182"
      },
      "source": [
        "Pre Processing"
      ],
      "id": "e5467182"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "133cd7f5"
      },
      "outputs": [],
      "source": [
        "df_train.shape"
      ],
      "id": "133cd7f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d90c65f8"
      },
      "outputs": [],
      "source": [
        "df_train.isnull().values.any()"
      ],
      "id": "d90c65f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2500d76c"
      },
      "outputs": [],
      "source": [
        "df_train['full_text'][0]"
      ],
      "id": "2500d76c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d62eabd2"
      },
      "outputs": [],
      "source": [
        "df_train['full_text'][5]"
      ],
      "id": "d62eabd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9cbf0e2"
      },
      "outputs": [],
      "source": [
        "df_train['full_text'][20]"
      ],
      "id": "b9cbf0e2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "845dff31"
      },
      "outputs": [],
      "source": [
        "df_train['full_text'] = df_train[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex=True)"
      ],
      "id": "845dff31"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ef4505"
      },
      "outputs": [],
      "source": [
        "df_train['full_text'][20]"
      ],
      "id": "43ef4505"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "012d928d"
      },
      "outputs": [],
      "source": [
        "df_train1=df_train.drop(['text_id','full_text'],axis=1).columns"
      ],
      "id": "012d928d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48b710c1"
      },
      "outputs": [],
      "source": [
        "for i in df_train1:\n",
        "    print(i)\n",
        "    print(df_train[i].value_counts().sort_values())"
      ],
      "id": "48b710c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0560cd8"
      },
      "outputs": [],
      "source": [
        "corpus = ''.join(df_train.full_text)\n",
        "d = Counter(corpus.split())\n",
        "d"
      ],
      "id": "d0560cd8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60b628c9"
      },
      "outputs": [],
      "source": [
        "words_counting = pd.DataFrame({'words':list(d.keys()), 'count':list(d.values())})\n",
        "words_counting['word_len'] = words_counting['words'].apply(lambda x: len(x))\n",
        "words_counting = words_counting.sort_values(by = 'word_len', ascending = False)\n",
        "words_counting"
      ],
      "id": "60b628c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b09dcd69"
      },
      "outputs": [],
      "source": [
        "print('Generic Sequences: ',words_counting[words_counting.words.str.contains('Generic')]['count'].sum())\n",
        "print('Generic Name Sequences: ',words_counting[words_counting.words.str.contains('Generic_Name')]['count'].sum())\n",
        "print('Generic City Sequences: ',words_counting[words_counting.words.str.contains('Generic_City')]['count'].sum())"
      ],
      "id": "b09dcd69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba3d7edf"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ],
      "id": "ba3d7edf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "621cfbe6"
      },
      "outputs": [],
      "source": [
        "eng_stopwords = nltk.corpus.stopwords.words('english')\n",
        "eng_stopwords"
      ],
      "id": "621cfbe6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcbf6eb4"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
        "    text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text)    \n",
        "    return text\n",
        "\n",
        "train_copy = df_train.copy()\n",
        "train_copy['full_text'] = train_copy['full_text'].map(lambda x: clean_text(x))"
      ],
      "id": "bcbf6eb4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14357903"
      },
      "outputs": [],
      "source": [
        "train_copy['full_text'][2]"
      ],
      "id": "14357903"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b33ad868"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def most_freq_ngrams(s1 = 2, s2 = 2):\n",
        "\n",
        "    vec = CountVectorizer(stop_words = eng_stopwords, ngram_range = (s1, s2))\n",
        "\n",
        "    bow = vec.fit_transform(train_copy[\"full_text\"])\n",
        "\n",
        "    count_values = bow.toarray().sum(axis=0)\n",
        "\n",
        "    ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in vec.vocabulary_.items()], reverse = True))\n",
        "    ngram_freq.columns = [\"frequency\", \"ngram\"]\n",
        "    \n",
        "    return ngram_freq"
      ],
      "id": "b33ad868"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ee015be"
      },
      "outputs": [],
      "source": [
        "figure, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (22, 6))\n",
        "for i in range(2,4):\n",
        "    ngram_freq = most_freq_ngrams(i, i)\n",
        "    sns.barplot(data = ngram_freq[:10], x = 'frequency', y = 'ngram', ax = axes[i-2])\n",
        "    del ngram_freq\n",
        "    \n",
        "figure.tight_layout(h_pad=1.0, w_pad=0.5)\n",
        "plt.suptitle('Frequent N-Grams', y=1.02)\n",
        "plt.show()"
      ],
      "id": "1ee015be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c5b841f"
      },
      "outputs": [],
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(8,4))\n",
        "avg = [df_train['cohesion'].mean(),df_train['syntax'].mean(),df_train['vocabulary'].mean(),df_train['phraseology'].mean(),df_train['grammar'].mean(),df_train['conventions'].mean()]\n",
        "bar = ['cohesion','syntax','vocabulary','phraseology','grammar','conventions']\n",
        "plt.bar(bar,avg,color='aqua')\n",
        "plt.title('Average Score of each metric')\n",
        "plt.xlabel('Grammar Metrics')\n",
        "plt.ylabel('Average Score')\n",
        "\n",
        "plt.show()"
      ],
      "id": "7c5b841f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7881e9c4"
      },
      "source": [
        "Model Building"
      ],
      "id": "7881e9c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a0c0625"
      },
      "outputs": [],
      "source": [
        "total_words = []\n",
        "for i in df_train['full_text']:\n",
        "    i = i.split()\n",
        "    total_words.append(len(i))\n",
        "\n",
        "df_train['total_words'] = total_words"
      ],
      "id": "3a0c0625"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17a3e038"
      },
      "outputs": [],
      "source": [
        "x = df_train['full_text']\n",
        "y = df_train.drop(['full_text','text_id','total_words'],axis=1)"
      ],
      "id": "17a3e038"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b419f52"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "id": "8b419f52"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca0a74b6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "a = []\n",
        "for i in x_train:\n",
        "    i = i.split()\n",
        "    for j in i:\n",
        "        a.append(j)\n",
        "len(set(a))"
      ],
      "id": "ca0a74b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2195c3bc"
      },
      "outputs": [],
      "source": [
        "text_vector = TextVectorization(max_tokens=37000,output_sequence_length=800)\n",
        "text_vector.adapt(x_train)"
      ],
      "id": "2195c3bc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da9fb948"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "embed = layers.Embedding(input_dim=37000,output_dim=128,input_length=800,mask_zero=True)"
      ],
      "id": "da9fb948"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0c1322c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "inputs = layers.Input(shape=(1,),dtype=tf.string)\n",
        "x = text_vector(inputs)\n",
        "x = embed(x)\n",
        "\n",
        "x = layers.LSTM(128,return_sequences=True)(x)\n",
        "x = layers.LSTM(128,return_sequences=True)(x)\n",
        "\n",
        "x = layers.LSTM(64,return_sequences=True)(x)\n",
        "x = layers.LSTM(64)(x)\n",
        "\n",
        "x = layers.Dense(128,activation='relu')(x)\n",
        "x = layers.Dropout(0.25)(x)\n",
        "x = layers.Dense(64,activation='relu')(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "\n",
        "output_1 = layers.Dense(1,activation='relu',name='o1')(x)\n",
        "output_2 = layers.Dense(1,activation='relu',name='o2')(x)\n",
        "output_3 = layers.Dense(1,activation='relu',name='o3')(x)\n",
        "output_4 = layers.Dense(1,activation='relu',name='o4')(x)\n",
        "output_5 = layers.Dense(1,activation='relu',name='o5')(x)\n",
        "output_6 = layers.Dense(1,activation='relu',name='o6')(x)\n",
        "\n",
        "model_lstm = tf.keras.Model(inputs=inputs,outputs=[output_1,output_2,output_3,output_4,output_5,output_6])"
      ],
      "id": "c0c1322c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f28eac27"
      },
      "outputs": [],
      "source": [
        "model_lstm.compile(loss={'o1':tf.keras.losses.MeanSquaredError(),\n",
        "                      'o2':tf.keras.losses.MeanSquaredError(),\n",
        "                      'o3':tf.keras.losses.MeanSquaredError(),\n",
        "                      'o4':tf.keras.losses.MeanSquaredError(),\n",
        "                      'o5':tf.keras.losses.MeanSquaredError(),\n",
        "                      'o6':tf.keras.losses.MeanSquaredError()},\n",
        "                optimizer='adam',\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])"
      ],
      "id": "f28eac27"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1533fc8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss',patience=5,verbose=1,mode='min')"
      ],
      "id": "d1533fc8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dab3ffcd"
      },
      "outputs": [],
      "source": [
        "valid_data = (x_test,y_test)\n",
        "model_lstm.fit(x=x_train,y=y_train,batch_size=32,epochs=10,validation_data=valid_data,callbacks=[early_stop])"
      ],
      "id": "dab3ffcd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10a620c9"
      },
      "outputs": [],
      "source": [
        "def prediction(model,x_test=x_test):\n",
        "    preds = model.predict(x_test)\n",
        "    preds = np.array(preds)\n",
        "    preds = preds.reshape((6,783))\n",
        "    return preds"
      ],
      "id": "10a620c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa95a2e3"
      },
      "outputs": [],
      "source": [
        "def answer(paragraph,model):\n",
        "    preds = model.predict(pd.Series(paragraph));\n",
        "    preds = np.array(preds)\n",
        "    preds.reshape((6))\n",
        "    print('cohesion:',float(preds[0]),\n",
        "        '\\nsyntax:',float(preds[1]),\n",
        "        '\\nvocabulary:',float(preds[2]),\n",
        "        '\\nphraseology:',float(preds[3]),\n",
        "        '\\ngrammar:',float(preds[4]),\n",
        "        '\\nconventions:',float(preds[5]))"
      ],
      "id": "aa95a2e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7deb326"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def evaluate(y_preds,y_true=y_test):\n",
        "    l1=[]\n",
        "    y_true = y_true.to_numpy()\n",
        "    for i in range(0,6):\n",
        "        error = mean_squared_error(y_true[:,i],y_preds[i])\n",
        "        error = error**0.5\n",
        "        l1.append(error)\n",
        "    return np.mean(l1)"
      ],
      "id": "f7deb326"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "557c66af"
      },
      "outputs": [],
      "source": [
        "preds=prediction(model_lstm)"
      ],
      "id": "557c66af"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4c8f42"
      },
      "outputs": [],
      "source": [
        "evaluate(preds)"
      ],
      "id": "7c4c8f42"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c94eac5a"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('test.csv')\n",
        "df_test.head()"
      ],
      "id": "c94eac5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2bed5f1"
      },
      "outputs": [],
      "source": [
        "preds = model_lstm.predict(df_test['full_text'])"
      ],
      "id": "a2bed5f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "966b01ef"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "  answer(df_test['full_text'][i],model_lstm)"
      ],
      "id": "966b01ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hSJK3MRVN3I"
      },
      "outputs": [],
      "source": [
        "essay_ID = input(\"Input your essay ID: \")\n",
        "essay_text = input(\"Input your essay text: \")\n",
        "data = {'text_id': essay_ID, 'full_text': essay_text}\n",
        "\n",
        "df_test = df_test.append(data, ignore_index=True)\n",
        "\n",
        "print(df_test)"
      ],
      "id": "3hSJK3MRVN3I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2FPobveWX_w"
      },
      "outputs": [],
      "source": [
        "answer(df_test['full_text'][3],model_lstm)"
      ],
      "id": "l2FPobveWX_w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2"
      ],
      "metadata": {
        "id": "PBDxttUgRvyd"
      },
      "id": "PBDxttUgRvyd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS3YjpRrd8aq"
      },
      "outputs": [],
      "source": [
        "features = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar',  'conventions']\n",
        "target = df_train[features]\n",
        "target"
      ],
      "id": "kS3YjpRrd8aq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgQ8ZxAPeA-J"
      },
      "outputs": [],
      "source": [
        "text_train = df_train['full_text']\n",
        "text_test = df_test['full_text']\n",
        "text = text_train.append(text_test)\n",
        "text"
      ],
      "id": "HgQ8ZxAPeA-J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP0MZy5kd10q"
      },
      "outputs": [],
      "source": [
        "y = target\n",
        "X = text[: len(df_train)]\n",
        "X_test = text[len(df_train) :]\n",
        "X.shape, X_test.shape, y.shape"
      ],
      "id": "UP0MZy5kd10q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeXO363jeLTC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=0.01)\n",
        "\n",
        "\n",
        "X_test = np.array(X_test).tolist()\n",
        "X_test = list(map(''.join, X_test))\n",
        "\n",
        "X_tfIdf = vectorizer_tfidf.fit_transform(X)\n",
        "\n",
        "X_test_tfIdf = vectorizer_tfidf.transform(X_test)\n",
        "print(vectorizer_tfidf.get_feature_names_out()[:5])"
      ],
      "id": "OeXO363jeLTC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1uXq5wEeOpS"
      },
      "outputs": [],
      "source": [
        "X_tfIdf.shape, X_test_tfIdf.shape, y.shape"
      ],
      "id": "T1uXq5wEeOpS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGNaW1ozeRGK"
      },
      "outputs": [],
      "source": [
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "chain = MultiOutputRegressor(SVR())\n",
        "chain.fit(X_tfIdf, y)\n",
        "print(chain.score(X_tfIdf,y))"
      ],
      "id": "rGNaW1ozeRGK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q8HmFpTeUS6"
      },
      "outputs": [],
      "source": [
        "predictions = chain.predict(X_test_tfIdf)\n",
        "predictions[3]"
      ],
      "id": "6Q8HmFpTeUS6"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
